<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Charles O&#39;Neill</title>
    <link>https://example.org/posts/</link>
    <description>Recent content in Posts on Charles O&#39;Neill</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-au</language>
    <copyright>Â© Charlie O&#39;Neill</copyright>
    <lastBuildDate>Fri, 01 Mar 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://example.org/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Can quantised autoencoders find and interpret circuits in language models?</title>
      <link>https://example.org/posts/vq_vae/</link>
      <pubDate>Fri, 01 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://example.org/posts/vq_vae/</guid>
      <description>Introduction Mechanistic interpretability has recently made significant breakthroughs in automatically identifying circuits in real-world language models like GPT-2. However, no one has yet been able to automatically interpret these circuits once found; instead, each circuit requires significant manual inspection before we can begin to guess what is going on.&#xA;This post explores the use of compression in the form of autoencoders in order to make the residual streams of transformers acting on certain tasks much more conducive to automated analysis.</description>
    </item>
  </channel>
</rss>
