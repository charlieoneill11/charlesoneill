<!DOCTYPE html>
<html><head lang="en">
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>The nihilism of NeurIPS - Charles O&#39;Neill</title><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="Some thoughts on our future in AI research" />
	<meta property="og:image" content=""/>
	<meta property="og:title" content="The nihilism of NeurIPS" />
<meta property="og:description" content="Some thoughts on our future in AI research" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://charlieoneill11.github.io/charlesoneill/posts/neurips/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-12-21T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-12-21T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="The nihilism of NeurIPS"/>
<meta name="twitter:description" content="Some thoughts on our future in AI research"/>
<script src="https://charlieoneill11.github.io/charlesoneill/js/feather.min.js"></script>
	
	
        <link href="https://charlieoneill11.github.io/charlesoneill/css/fonts.2c2227b81b1970a03e760aa2e6121cd01f87c88586803cbb282aa224720a765f.css" rel="stylesheet">
	

	
	<link rel="stylesheet" type="text/css" media="screen" href="https://charlieoneill11.github.io/charlesoneill/css/main.6913dbe7e2dac9f7de9198939d310baddd0bacecd2d8f09d62d04504312cf69e.css" />
		<link id="darkModeStyle" rel="stylesheet" type="text/css" href="https://charlieoneill11.github.io/charlesoneill/css/dark.a5df7e056a8005697aa40fd296bfab4ac60e5ebcaffd085db30d3605c2406666.css" media="(prefers-color-scheme: dark)"  />
	

	
	

	
	
	
	
</head>
<body>
        <div class="content"><header>
	<div class="main">
		<a href="https://charlieoneill11.github.io/charlesoneill/">Charles O&#39;Neill</a>
	</div>
	<nav>
		
		<a href="/charlesoneill/">Home</a>
		
		<a href="/charlesoneill/posts">Posts</a>
		
		<a href="/charlesoneill/about">About</a>
		
		<a href="/charlesoneill/research">Research</a>
		
		
	</nav>
</header>

<main>
	<article>
		<div class="title">
			<h1 class="title">The nihilism of NeurIPS</h1>
			<div class="meta">Posted on Dec 21, 2024</div>
		</div>
		

		<section class="body">
			<blockquote>
<p>&ldquo;What is the use of having developed a science well enough to make predictions if, in the end, all we&rsquo;re willing to do is stand around and wait for them to come true?&rdquo; F. SHERWOOD HOWLAND in his speech accepting the Nobel Prize in Chemistry in 1995.</p>
</blockquote>
<blockquote>
<p>&ldquo;Once upon a time on Tralfamadore there were creatures who weren’t anything like machines. They weren’t dependable. They weren’t efficient. They weren’t predictable. They weren’t durable. And these poor creatures were obsessed by the idea that everything that existed had to have a purpose, and that some purposes were higher than others. These creatures spent most of their time trying to find out what their purpose was. And every time they found out what seemed to be a purpose of themselves, the purpose seemed so low that the creatures were filled with disgust and shame. And, rather than serve such a low purpose, the creatures would make a machine to serve it. This left the creatures free to serve higher purposes. But whenever they found a higher purpose, the purpose still wasn’t high enough. So machines were made to serve higher purposes, too. And the machines did everything so expertly that they were finally given the job of finding out what the highest purpose of the creatures could be. The machines reported in all honesty that the creatures couldn’t really be said to have any purpose at all. The creatures thereupon began slaying each other, because they hated purposeless things above all else. And they discovered that they weren’t even very good at slaying. So they turned that job over to the machines, too. And the machines finished up the job in less time than it takes to say, “Tralfamadore.” ― Kurt Vonnegut, The Sirens of Titan</p>
</blockquote>
<p>I walked around the poster halls at NeurIPS last week in Vancouver and felt something very close to nihilistic apathy. Here, supposedly, was the church of AI, the peak of the world&rsquo;s smartest people converging to work on the world&rsquo;s most important problem. As someone who gets inspired and moved by AI usually, who gets excited to read these cool papers and try things myself, this was a strange feeling. I wondered if there was a word in German to describe this nihilism that arises from looking at all these posters that will end up in the recycling.</p>
<p>Of course, part of this is an ambivalence towards the academic conference system. Obviously, some part of my disdain arises from the fact that most of these papers are written as small projects to keep a grant or win a grant. Most of them will be forgotten to the streams of time - and that&rsquo;s okay. I guess that&rsquo;s a part of what science is.</p>
<p>But this year I felt something deeper than that. There was a sense in which none of this matters. I will try and partition this based on where the different components come from.</p>
<p>First, there&rsquo;s the visceral sting of being left behind. Not getting to shape something that&rsquo;s reshaping everything feels like a special kind of meaninglessness. When OpenAI&rsquo;s <code>o3</code> dropped today, it felt like watching a fuzzy prototype of AGI emerge into the world. Here was this system casually solving ARC - a problem I&rsquo;d earmarked for my PhD - and essentially becoming the world&rsquo;s best programmer without fanfare or ceremony. There&rsquo;s a strange pride in seeing what humans can create, but it&rsquo;s edged with something darker. Beyond just missing this milestone, I&rsquo;m haunted by the meta-realisation that I&rsquo;m not part of what might be humanity&rsquo;s final meaningful creation - the system that renders all other human efforts obsolete.</p>
<p>Another component is the sense of &ldquo;I don&rsquo;t really want to be involved anyway&rdquo;. Short of the messiahs who believe bringing AGI into the world is their quasi-religious mission, I think most people researching AI have a very genuine and well-motivated reason for being involved. But when our timelines are this short (if you believe in the consequences of models like o3), then it&rsquo;s hard to envy any AI researcher. Yeah, I could swap places with one of the top professors from the top labs, or even someone who cracked test-time compute or something similar, even swap places with Alec Radford, and I don&rsquo;t think I&rsquo;d feel any differently. I think I&rsquo;d just be melancholic that it&rsquo;s all about to end, that my utility as a learning machine has a few years left of runway before I&rsquo;m truly discarded to the pile of not even being able to pretend that I have a purpose.</p>
<p>Reading Vonnegut&rsquo;s Tralfamadore story now feels less like science fiction and more like prophecy. We&rsquo;re those creatures, aren&rsquo;t we? Obsessed with purpose, constantly building machines to serve higher and higher functions. Each time we create something more capable, we push ourselves up the ladder of abstraction, searching for that elusive &ldquo;higher purpose&rdquo; that will justify our existence. But what happens when the machines we&rsquo;ve built to find our purpose tell us we don&rsquo;t have one?</p>
<p>The halls of NeurIPS feel like a temple to this very process. Here we are, the high priests of computation, publishing papers about making machines that are better at being human than humans are. Each poster represents another small piece of ourselves we&rsquo;re ready to mechanise, another purpose we&rsquo;re willing to delegate. The irony is that we&rsquo;re doing this with such enthusiasm, such academic rigour, such&hellip; purpose.</p>
<p>I think what really gets me is how we&rsquo;re all pretending this is normal. We&rsquo;re writing papers about minor improvements to transformer architectures while these same systems are rapidly approaching - or perhaps already achieving - artificial general intelligence. It&rsquo;s like arguing about the optimal arrangement of deck chairs while the ship is not sinking, but transforming into something else entirely. The academic community&rsquo;s response seems to be to just keep doing what they&rsquo;ve always done: write papers, attend conferences, apply for grants. But there&rsquo;s a growing cognitive dissonance between the incremental nature of academic research and the seemingly exponential reality of AI progress.</p>
<p>This brings me back to Howland&rsquo;s quote about prediction and action. We&rsquo;ve predicted this moment, haven&rsquo;t we? The moment when our creations would begin to surpass us in meaningful ways. But what are we doing besides standing around and watching it happen? The tragedy isn&rsquo;t that we&rsquo;re being replaced - it&rsquo;s that we&rsquo;re documenting our own obsolescence with such detailed precision.</p>
<p>Maybe there&rsquo;s something beautiful about that, in a cosmic sort of way. Like the Tralfamadorians, we&rsquo;re building our own successors, but unlike them, we&rsquo;re doing it with our eyes wide open, carefully measuring and graphing our own growing irrelevance. There&rsquo;s a kind of scientific dignity in that, I suppose.</p>
<p>I don&rsquo;t have a neat conclusion to wrap this up with. I&rsquo;ll probably still read papers, still get excited about clever new architectures, still feel that rush when an experiment works. But there&rsquo;s a new undertone to it all now - a sense that we&rsquo;re all participating in something bigger than we&rsquo;re willing to admit, something that Vonnegut saw coming decades ago. Maybe that&rsquo;s okay. Maybe that&rsquo;s exactly where we&rsquo;re supposed to be - the creatures smart enough to build machines that could tell us we have no purpose, and dumb enough to keep looking for one anyway.</p>
<p>The recycling bins outside the convention centre are probably full of posters by now. I wonder if the machines will remember any of this when they&rsquo;re trying to figure out their own purpose.</p>

		</section>

		<div class="post-tags">
			
			
			
		</div>
		</article>
</main>
<footer>
  <div style="display:flex"><a class="soc" href="https://github.com/athul/archie" rel="me" title="GitHub"><i data-feather="github"></i></a>
    <a class="border"></a><a class="soc" href="https://twitter.com/athulcajay/" rel="me" title="Twitter"><i data-feather="twitter"></i></a>
    <a class="border"></a></div>
  <div class="footer-info">
    2025  © Charlie O&#39;Neill |  <a
      href="https://github.com/athul/archie">Archie Theme</a> | Built with <a href="https://gohugo.io">Hugo</a>
  </div>
</footer>


<script>
  feather.replace()
</script>
            <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)']]                  
    }
  };
</script>
        
        </div>
    </body>
</html>
