---
title: SAErch: retrieval with sparse autoencoders
date: 2024-07-10
description: Reimagining search tools with SAE steering
---

Building tools that give users intuitive control over latent and semantic aspects of the space they're interacting with is now sort of possible with sparse autoencoders (SAEs). SAEs are a type of neural network that learn a sparse, higher-dimensional representation of a continuous, superimposed, messy input space. Because our learned representations are sparse, we can view individually directions in this sparse basis as meaningful features that the input space is using. One way to think of this is disentangling features superimposed on top of each other in the input space (because there are more features than orthogonal directions); we learn how to disentangle these with dictionary learning. So far, people have successfully applied these to the internal activations of language models, to find features in this space that we can interpret and even use to causally steer the model (REFERENCE). Recently, [thesephist applied SAEs to text embeddings](https://thesephist.com/posts/prism/#training-sparse-autoencoders), and used learned SAE features to intervene on different features in embeddings and decode these back to text, thus providing a way to control certain aspects of a text. People have even done similar things with [CLIP embeddings for stable diffusion generation control](https://x.com/YondonFu/status/1805319701947371710).

However, using SAEs for embeddings gave rise to an interesting idea: can we use SAEs to provide more fine-grained control over semantic search? At the moment, semantic search roughly looks like (1) embedding your query, and then (2) computing the cosine similarity with all embeddings of documents in your vector database. However, its hard to quantitatively and precisely control the semantics of the query. Embeddings pick up weird things. What if, with SAE features, we could provide sliders that allow you to upweight or downweight specific topics, concepts or even just fuzzy semantic latents?

